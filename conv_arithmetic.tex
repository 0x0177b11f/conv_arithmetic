\documentclass{article}
\usepackage{amsmath,amsfonts,amsthm}
\usepackage{authblk}
\usepackage[T1]{fontenc}
\usepackage[letterpaper, margin=1.5in]{geometry}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{natbib}
\usepackage{xcolor}

\usepackage{hyperref}

\definecolor{red}{RGB}{220,50,47}

\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}

\newtheorem{prop}{Property}
\providecommand*{\propautorefname}{Property}

\title{A guide to convolution arithmetic for deep learning}
\author[1]{Vincent Dumoulin\thanks{dumouliv@iro.umontreal.ca}}
\author[1]{Francesco Visin\thanks{fvisin@gmail.com}}
\affil[1]{MILA, Universit\'{e} de Montr\'{e}al}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
\todo{WRITEME}
\end{abstract}

\section{Introduction}

Deep convolutional neural networks (CNNs) have been at the heart of spectacular
advances in deep learning. Although CNNs have been used as early as the nineties
\citep{lecun1998gradient} to solve character recognition tasks, their current
widespread application is due to much more recent work, when a deep CNN was used
to beat state-of-the-art in the ImageNet image classification challenge
\citep{krizhevsky2012imagenet}.

Convolutional neural networks therefore constitute a very useful tool for
machine learning practitioners. However, learning to use CNNs for the first time
is generally an intimidating experience. A convolutional layer's output shape is
affected by the shape of its input as well as the choice of kernel shape, zero
padding and strides, and the relationship between these properties is not
trivial to infer. This contrasts with fully-connected layers, whose output size
is independent of its input size.

Additionally, so-called transposed convolutional layers (also known as
fractionally strided convolutional layers) have been employed in more and more
work as of late, and their relationship with convolutional layers has been
explained with various degrees of clarity.

This guide's objective is twofold:

\begin{enumerate}
    \item Explain the relationship between convolutional layers and transposed
        convolutional layers.
    \item Provide an intuitive understanding of the relationship between input
        shape, kernel shape, zero padding, strides and output shape in
        convolutional layers and transposed convolutional layers.
\end{enumerate}

\subsection{Discrete convolutions}

The bread and butter of neural networks is affine transformations: a vector is
received as input and is multiplied with a matrix to produce an output (to which
a bias vector is usually added before passing the result through a
nonlinearity). This is applicable to any type of input, be it an image, a sound
clip or an unordered collection of features: their representation can always be
flattened into a vector before the transformation.

In the case of images and sound clips, there is an ordering (spatial or
temporal) being thrown away when they are flattened. This information may prove
very handy in solving some tasks, like computer vision and speech recognition.

These sorts of inputs share important properties:

\begin{itemize}
    \item They are stored as multi-dimensional arrays.
    \item They feature one or more axes for which ordering matters (e.g. width
        and height axes for an image, time axis for a sound clip).
    \item One axis, called the channel axis, is used to access different views
        of the data (e.g. the red, green and blue channels of a color image, or
        the left and right channels of a stereo audio track).
\end{itemize}

A discrete convolution is a linear transformation that is sparse (only a few
input units contribute to a given output unit) and reuses parameters (the same
transformation is applied to multiple locations in the input).

\autoref{fig:numerical_no_padding_no_strides} provides an example of a discrete
convolution. The blue grid is called the input {\em feature map}\footnote{
    What we referred to earlier as {\em channels} is a name for the {\em feature
    maps} of input data.}.
We slide a {\em kernel} (dark blue) of value

\begin{equation}
\begin{pmatrix}
    0 & 1 & 2 \\
    2 & 2 & 0 \\
    0 & 1 & 2
\end{pmatrix}
\end{equation}

across the input feature map. At each location, we compute the product between
the kernel elements and the overlapped input elements and sum the results up
to obtain the corresponding output {\em feature map} value\footnote{
    While there is a distinction between convolution and cross-correlation from
    a signal processing perspective, the two become interchangeable when the
    kernel is learned. For the sake of simplicity and to stay consistent with
    most of the machine learning litterature, the term {\em convolution}
    will be used in this guide.}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.3\textwidth]
        {pdf/numerical_no_padding_no_strides_00.pdf}
    \includegraphics[width=0.3\textwidth]
        {pdf/numerical_no_padding_no_strides_01.pdf}
    \includegraphics[width=0.3\textwidth]
        {pdf/numerical_no_padding_no_strides_02.pdf}
    \caption{\label{fig:numerical_no_padding_no_strides} Computing three output
        values of a discrete convolution.}
\end{figure}

If there are multiple input feature maps, each input feature map is convolved
with a distinct kernel, and the resulting feature maps are summed up elementwise
to produce the output feature map. The procedure can be repeated using different
kernels to form as many output feature maps as desired.

The convolution presented in \autoref{fig:numerical_no_padding_no_strides} is an
instance of a 2-D convolution, but it can be generalized to N-D convolutions.
For instance, in a 3-D convolution, the kernel would be a {\em cuboid} and would
slide across the height, width and depth of the input feature map.

The collection of kernels defining a discrete convolution has a shape
corresponding to some permutation of $(n, m, k_1, \ldots, k_N)$, where

\begin{equation}
\begin{split}
    n &\equiv \text{number of output feature maps},\\
    m &\equiv \text{number of input feature maps},\\
    k_j &\equiv \text{kernel size along axis $j$}.
\end{split}
\end{equation}

The following properties affect the output size $o_j$ of a convolutional layer
along axis $j$:

\begin{itemize}
    \item $i_j$: input size along axis $j$,
    \item $k_j$: kernel size along axis $j$,
    \item $s_j$: stride (distance between two consecutive positions of the
        kernel) along axis $j$,
    \item $p_j$: zero-padding (number of zeros concatenated at the beginning and
        at the end of an axis) along axis $j$.
\end{itemize}

For instance, \autoref{fig:numerical_padding_strides} shows a $3 \times 3$
kernel applied to a $5 \times 5$ input padded with a $1 \times 1$ border of
zeros using $2 \times 2$ strides.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.3\textwidth]{pdf/numerical_padding_strides_00.pdf}
    \includegraphics[width=0.3\textwidth]{pdf/numerical_padding_strides_01.pdf}
    \includegraphics[width=0.3\textwidth]{pdf/numerical_padding_strides_02.pdf}
    \caption{\label{fig:numerical_padding_strides} Computing three output values
        of a discrete convolution for $N = 2$, $i_1 = i_2 = 5$, $k_1 = k_2 = 3$,
        $s_1 = s_2 = 2$, and $p_1 = p_2 = 1$.}
\end{figure}

The analysis of the relationship between these parameters is simplified by the
fact that they don't interact across axes, i.e. choices of kernel size, stride
and zero padding along axis $j$ only affect the output size of axis $j$.

Because of this, the guide will concentrate on 2-D convolutional layers acting
on square inputs, and the kernel size, stride and zero-padding will always be
the same along both axes.

\section{Convolution arithmetic}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.2\textwidth]{pdf/no_padding_no_strides_00.pdf}
    \includegraphics[width=0.2\textwidth]{pdf/no_padding_no_strides_01.pdf}
    \includegraphics[width=0.2\textwidth]{pdf/no_padding_no_strides_02.pdf}
    \includegraphics[width=0.2\textwidth]{pdf/no_padding_no_strides_03.pdf}
    \caption{\label{fig:no_padding_no_strides} Convolving a $3 \times 3$ kernel
        over a $4 \times 4$ input using unit strides (i.e. $i = 4$, $k = 3$,
        $s = 1$ and $p = 0$).}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.2\textwidth]
        {pdf/arbitrary_padding_no_strides_00.pdf}
    \includegraphics[width=0.2\textwidth]
        {pdf/arbitrary_padding_no_strides_01.pdf}
    \includegraphics[width=0.2\textwidth]
        {pdf/arbitrary_padding_no_strides_02.pdf}
    \includegraphics[width=0.2\textwidth]
        {pdf/arbitrary_padding_no_strides_03.pdf}
    \caption{\label{fig:arbitrary_padding_no_strides} Convolving a $4 \times 4$
        kernel over a $5 \times 5$ input padded with a $2 \times 2$ border of
        zeros using unit strides (i.e. $i = 5$, $k = 4$, $s = 1$ and $p = 2$).}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.2\textwidth]{pdf/same_padding_no_strides_00.pdf}
    \includegraphics[width=0.2\textwidth]{pdf/same_padding_no_strides_01.pdf}
    \includegraphics[width=0.2\textwidth]{pdf/same_padding_no_strides_02.pdf}
    \includegraphics[width=0.2\textwidth]{pdf/same_padding_no_strides_03.pdf}
    \caption{\label{fig:same_padding_no_strides} Convolving a $3 \times 3$
        kernel over a $5 \times 5$ input using half padding and unit strides
        (i.e. $i = 5$, $k = 3$, $s = 1$ and $p = 1$).}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.2\textwidth]{pdf/full_padding_no_strides_00.pdf}
    \includegraphics[width=0.2\textwidth]{pdf/full_padding_no_strides_01.pdf}
    \includegraphics[width=0.2\textwidth]{pdf/full_padding_no_strides_02.pdf}
    \includegraphics[width=0.2\textwidth]{pdf/full_padding_no_strides_03.pdf}
    \caption{\label{fig:full_padding_no_strides} Convolving a $3 \times 3$
        kernel over a $5 \times 5$ input using full padding and unit strides
        (i.e. $i = 5$, $k = 3$, $s = 1$ and $p = 2$).}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.2\textwidth]{pdf/no_padding_strides_00.pdf}
    \includegraphics[width=0.2\textwidth]{pdf/no_padding_strides_01.pdf}
    \includegraphics[width=0.2\textwidth]{pdf/no_padding_strides_02.pdf}
    \includegraphics[width=0.2\textwidth]{pdf/no_padding_strides_03.pdf}
    \caption{\label{fig:no_padding_strides} Convolving a $3 \times 3$
        kernel over a $5 \times 5$ input using $2 \times 2$ strides (i.e.
        $i = 5$, $k = 3$, $s = 2$ and $p = 0$).}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.2\textwidth]{pdf/padding_strides_00.pdf}
    \includegraphics[width=0.2\textwidth]{pdf/padding_strides_01.pdf}
    \includegraphics[width=0.2\textwidth]{pdf/padding_strides_02.pdf}
    \includegraphics[width=0.2\textwidth]{pdf/padding_strides_03.pdf}
    \caption{\label{fig:padding_strides} Convolving a $3 \times 3$ kernel over a
        $5 \times 5$ input padded with a $1 \times 1$ border of zeros using $2
        \times 2$ strides (i.e.  $i = 5$, $k = 3$, $s = 2$ and $p = 1$).}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.2\textwidth]{pdf/padding_strides_odd_00.pdf}
    \includegraphics[width=0.2\textwidth]{pdf/padding_strides_odd_01.pdf}
    \includegraphics[width=0.2\textwidth]{pdf/padding_strides_odd_02.pdf}
    \includegraphics[width=0.2\textwidth]{pdf/padding_strides_odd_03.pdf}
    \caption{\label{fig:padding_strides_odd} Convolving a $3 \times 3$ kernel
        over a $6 \times 6$ input padded with a $1 \times 1$ border of zeros
        using $2 \times 2$ strides (i.e.  $i = 6$, $k = 3$, $s = 2$ and
        $p = 1$). In this case, the bottom row and right column of the
        zero-padded input are not covered by the kernel.}
\end{figure}

\subsection{No zero-padding, unit strides}

The simplest case to analyze is when a kernel just slides across the input (i.e.
$s = 1$ and $p = 0$). \autoref{fig:no_padding_no_strides} provides an example
for $i = 4$ and $k = 3$.

One way of defining the output size is by the number of possible placements of
the kernel on the input. Consider the width axis: the left side of the kernel
starts at the left side of the input, and the kernel slides by steps of one
until its right side touches the right side of the input. The size of the output
is equal to the number of steps made, plus one, accounting for the initial
position of the kernel. The same logic applies for the height axis. From this,
the following property is inferred:

\begin{prop}\label{prop:no_padding_no_strides}
For $i$, $k$, $s = 1$ and $p = 0$,

\begin{equation}
    o = (i - k) + 1.
\end{equation}
\end{prop}

\subsection{Zero-padding, unit strides}

To factor in zero-padding (i.e. only restricting that $s = 0$), consider its
effect on the effective input size. \autoref{fig:arbitrary_padding_no_strides}
provides an example for $i = 5$, $k = 4$ and $p = 2$.

Noting that padding with $p$ zeros changes the effective input size from $i$ to
$i + 2p$, \autoref{prop:no_padding_no_strides} is used to infer the following
property:

\begin{prop}\label{prop:arbitrary_padding_no_strides}
For $i$, $k$, $s = 1$ and $p$,

\begin{equation}
    o = (i - k) + 2p + 1.
\end{equation}
\end{prop}

Two specific instances of zero padding are used quite extensively in practice
because of their respective properties.

\subsubsection{Half padding}

Having the output size be the same as the input size can be a desireable
property:

\begin{prop}\label{prop:same_padding_no_strides}
If $k$ is odd ($k = 2n + 1, \quad n \in \mathbb{N}$), and for $i$, $s = 1$ and
$p = \lfloor k / 2 \rfloor = n$,

\begin{equation}
\begin{split}
    o &= i + 2 \lfloor k / 2 \rfloor - (k - 1) \\
      &= i + 2n - 2n \\
      &= i.
\end{split}
\end{equation}
\end{prop}

This is sometimes referred to as {\em half} (or {\em same}) padding.
\autoref{fig:same_padding_no_strides} provides an example for $i = 5$, $k = 3$
and (therefore) $p = 1$.

\subsubsection{Full padding}

While convolving a kernel generally {\em decreases} the output size with respect
to the input size, sometimes the opposite is required:

\begin{prop}\label{prop:full_padding_no_strides}
If $p = k - 1$, and for $i$, $k$ and $s = 1$,

\begin{equation}
\begin{split}
    o &= i + 2(k - 1) - (k - 1) \\
      &= i + (k - 1).
\end{split}
\end{equation}
\end{prop}

This is sometimes referred to as {\em full} padding, because in this setting
every element of the kernel is superimposed on every element of the input
feature map. \autoref{fig:full_padding_no_strides} provides an example for $i =
5$, $k = 3$ and (therefore) $p = 2$.

\subsection{No zero-padding, non-unit strides}

So far, all properties derived only apply for unit-strided convolutions.
Incorporating non-unit strides requires another inference leap. To facilitate
the analysis, let's momentarily ignore zero-padding (i.e.  $s > 1$ and $p = 0$).
\autoref{fig:no_padding_strides} provides an example for $i = 5$, $k = 3$ and $s
= 2$.

Once again, the output size is defined in terms of the number of possible
placements of the kernel on the input. Consider the width axis: the left side of
the kernel starts at the left side of the input, but this time the kernel slides
by steps of $s$ until its right side touches the right side of the input. The
size of the output is equal to the number of steps made, plus one, accounting
for the initial position of the kernel. The same logic applies for the height
axis. From this, the following property is inferred:

\begin{prop}\label{prop:no_padding_strides}
For $i$, $k$, $p = 0$ and $s$,

\begin{equation}
    o = \left\lfloor \frac{i - k}{s} \right\rfloor + 1.
\end{equation}
\end{prop}

The floor function accounts for the fact that sometimes the last possible step
does {\em not} coincide with the kernel reaching the end of the input, i.e.
some input units are left out (see \autoref{fig:padding_strides_odd} for an
example of such a case).

\subsection{Zero-padding, non-unit strides}

The most general case (convolving over a zero-padded input using non-unit
strides) is analyzed by applying \autoref{prop:no_padding_strides} on an
effective input of size $i + 2p$, just as was done for
\autoref{prop:arbitrary_padding_no_strides}:

\begin{prop}\label{prop:padding_strides}
For $i$, $k$, $p$ and $s$,

\begin{equation}
    o = \left\lfloor \frac{i + 2p - k}{s} \right\rfloor + 1.
\end{equation}
\end{prop}

\autoref{fig:padding_strides} provides an example for $i = 5$, $k = 3$, $s = 2$
and $p = 1$.

As was noted for \autoref{prop:no_padding_strides}, the floor function means
that in some cases a convolution will produce the same output size for multiple
input sizes.

More specifically, if $i + 2p - k$ is a multiple of $s$, then any input size $j
= i + a, \quad a \in \{0,\ldots,s - 1\}$ will produce the same output size. Note
that this ambiguity applies only for $s > 1$.

\autoref{fig:padding_strides_odd} provides an example for $i = 6$, $k = 3$, $s =
2$ and $p = 1$. Despite having different input sizes, the convolutions presented
in \autoref{fig:padding_strides} and \autoref{fig:padding_strides_odd} share the
same output size.

While this doesn't affect the analysis for {\em convolutions}, this will
complicate the analysis in the case of {\em transposed convolutions}.

\bibliography{bibliography}
\bibliographystyle{natbib}
\end{document}
